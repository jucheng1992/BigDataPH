---
title: "Using R on AWS"
author: "Cheng Ju"
date: "3/24/2015"
output: html_document
---

#Connect to instance


After changing into the aws file, you will find file called Vagrantfile. You can change the remote machine you use by modify this file. You have three options and you can choos one by make '#' comment on the other two lines. Also you can list other kind computers in this text file.

Then use command `vagrant up` and then `vagrant ssh` to connect to the instance you created.

And to check the status of the instance, you can log in to the website:
https://ph290-2015-spring-uq6b1f1.signin.aws.amazon.com/console

#Install R and Rstudio on server

##Installing Base R
Create a user, home directory and set password

`sudo useradd rstudio`

`sudo mkdir /home/rstudio`

`sudo passwd rstudio`

`sudo chmod -R 0777 /home/rstudio`

update all files from the default state:

`sudo apt-get update`

`sudo apt-get upgrade`
 
Then add CRAN mirror to custom sources.list file using vi (optional)

`sudo vi /etc/apt/sources.list.d/sources.list`
 
Then add following line (or your favorite CRAN mirror)

`deb http://lib.stat.cmu.edu/R/CRAN/bin/linux/ubuntu precise/`
 
Update files to use CRAN mirror (Don't worry about error message)

`sudo apt-get update`
 
Install latest version of R

`sudo apt-get install r-base`

##Installing R-studio

With these commands, you will now be able to run R from the command line just by typing “R” at the prompt. We can also install R-studio, which makes working in R so much easier.

Fierst, we need to install a few background files

`sudo apt-get install gdebi-core`

`sudo apt-get install libapparmor1`
 
Then we change to a writeable directory. Download & Install RStudio Server

`cd /tmp`

`wget http://download2.rstudio.org/rstudio-server-0.97.336-amd64.deb`

`sudo gdebi rstudio-server-0.97.336-amd64.deb`


#Installing MySQL, MongoDB, Emacs

Also you can install other packages by the similar method.

```
#Install MySQL
sudo apt-get install mysql-common
sudo apt-get install mysql-server
#Install MongoDB
sudo apt-get install mongodb
#Install Emacs
sudo apt-get install emacs
```

#Download data and code from website

If you familiar with unix command, this part is very simple. I suggest to write code on your own machine using small chunk of the data. After making sure your code is correct, you can transport your code and data to the instance.

After creat your instance, you can use command `lscpu` to see the performence of the instance.

Here we just use the example from parallel programming: use the shared memory parallel programming on EC2.

You can direct download dataset and code from internet using 'wget' command. For example, you can down load code from my github:

`wget "https://github.com/jucheng1992/BigDataPH/blob/master/Parallel_R/GLMParallel.R"`

Then we can run the code by:

`R CMD BATCH GLMParallel.R GLMParallel.Rout`

The last second parameter "GLMParallel.R" means you want to run this file. The last parameter means you want to store your result in "GLMParallel.R"

Then you may want to find your result: 'cat GLMParallel.Rout', you will see the details of your reuslt:
```
ptime
   user  system elapsed 
  4.479   0.163  23.234 
> stime
   user  system elapsed 
 41.534   0.000  41.529 
> 
> proc.time()
   user  system elapsed 
 46.384   0.172  65.466 
```



#Data analysis using EC2

Here is example of how to do simple data analysis one EC2. We want to some daya analysis using the data from UCI machine learning repository. And we need to analyze the Wine Quality Data Set. 

First, we download data from data repository. We just open the website https://archive.ics.uci.edu/ml/datasets/Wine+Quality

And copy the link address, download it using command line:

`wget "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"`

Then we read our data:

```{r}
winedata<-read.csv2("winequality-white.csv", header = TRUE)

head(winedata)

```

This is a classfication problem. So maybe we can try using random forest. We can also use doParallel package to do the parallel programming. As we can request compueter with four cores, we can set number of nodes equals 4. At last, we want to know which feature is the most important. 


```{r}
library(doParallel)
library(randomForest)

##change factor into numerical data
curr<-data.matrix(winedata)

prf.time<-system.time({
      cl <- makeCluster(4)
      registerDoParallel(cl)
      ## each cluster, we set 2500 trees
      rf <- foreach(ntree=rep(250, 4), 
                    .combine=combine,
                    .packages='randomForest') %dopar%
            randomForest(as.factor(quality)~., data=curr, ntree=ntree)
      stopCluster(cl)
})

srf.time<-system.time(
      srf<-randomForest(as.factor(quality)~., curr, ntree=1000)
)

prf.time
srf.time
summary(rf)
importance(rf)

```

This is the result complied by my own computer. Then we can upload this R file to AWS instance. Then just use command `R CMD BATCH Winedata.R RF.Rout` to run the parallelized RF on instance. Then we use `cat RF.Rout` to look at the result.

```

> prf.time
   user  system elapsed 
  1.198   0.293   6.129 
> srf.time
   user  system elapsed 
  9.182   0.097   9.278 

```
##Cross validation

After doing some analysis, we may want to do cross-validation to make sure we did not overfit. Here we can also write function to do parallelized  cross validation. Here is an example to do parallelized k-fold cross-validation.

First, we write a function to split the data into k trunks. The input is the data.The out put is a list, with the k-th element to be the index of the k-th fold.

```{r}

dataSplit = function(data, k, random.seed = NULL) {
    # Set seed to reproduce the same result
    if (!is.null(random.seed)) 
        set.seed(random.seed)

    n = nrow(data)
    m = n%/%k
    index = 1:n

    # Split into k folds
    ans = vector(k, mode = "list")
    for (i in 1:(k - 1)) {
        ans[[i]] = sample(index, m)
        # move index out of our candidate index set
        index = setdiff(index, ans[[i]])
    }
    ans[[k]] = index
    return(ans)
}

```


Then we write cross-validation function. Here we want to do CV for random frest. The input is the data, the number of fold, the formula, and cv.index (which comes from datasplit function). The out put is just the model that only trained by the training data.


```{r}

crossValidation = function(data, cv.index, i, formula.text) {
    n = nrow(data)
    # Use k_th fold as test set
    test.ind = cv.index[[i]]
    # Use the rest of data as training set
    train.ind = setdiff(1:n, test.ind)
    train.data = data[train.ind, ]
    test.data = data[test.ind, ]

    rp.model = randomForest(as.formula(formula.text), data = train.data,ntree=1000)
    rp.model
}


```


Here we just apply the datasplit and crossValidation functions to our wine dataset.

```{r}
cv.index = dataSplit(data = winedata, k = 5, random.seed = 128)
k = 5
cv.model = vector(k, mode = "list")
scv.time<-system.time({
    for (i in 1:k) {
        cv.model[[i]] = crossValidation(curr, cv.index, i, "as.factor(quality)~.")
    }
})


cl <- makeCluster(4)
registerDoParallel(cl)
pcv.time<-system.time({
    cv.model <- foreach(i = 1:k, .packages = c("randomForest")) %dopar% {
        crossValidation(curr, cv.index, i, "as.factor(quality)~.")
    }
})
stopCluster(cl)

scv.time
pcv.time
```

Similarly, we can put our file on the AWS, use the similar command.



